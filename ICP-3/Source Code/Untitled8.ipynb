{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Muo7FEH5xNuu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXgM3RCUxPgY",
        "outputId": "de38a3f4-b8bf-4209-d83c-b9349f855a98"
      },
      "source": [
        " from nltk.corpus import wordnet\r\n",
        " import nltk\r\n",
        " nltk.download('wordnet')\r\n",
        " # lets use word paint as an exqmple\r\n",
        "syns = wordnet.synsets(\"kansas\")\r\n",
        "\r\n",
        "# An example of a synset:\r\n",
        "print(syns[0].name())\r\n",
        "print('\\n')\r\n",
        "# Just the word:\r\n",
        "print(syns[0].lemmas()[0].name())\r\n",
        "print('\\n')\r\n",
        "\r\n",
        "# Definition of that first synset:\r\n",
        "print(syns[0].definition())\r\n",
        "print('\\n')\r\n",
        "# Examples of the word in use in sentences:\r\n",
        "print(syns[0].examples())\r\n",
        "print('\\n')\r\n",
        "\r\n",
        "# synonyms and antonyms using wordnet using word\r\n",
        "synonyms = []\r\n",
        "antonyms = []\r\n",
        "\r\n",
        "for syn in wordnet.synsets(\"kansas\"):\r\n",
        "    for l in syn.lemmas():\r\n",
        "        synonyms.append(l.name())\r\n",
        "        if l.antonyms():\r\n",
        "            antonyms.append(l.antonyms()[0].name())\r\n",
        "print('The synonyms of kansas are: ')\r\n",
        "print(set(synonyms))\r\n",
        "print('\\n')\r\n",
        "print('The antonyms of kansas are: ')\r\n",
        "print(set(antonyms))\r\n",
        "print('\\n')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# comparison/ similarity score between 2 words\r\n",
        "w1 = wordnet.synset('ship.n.01')\r\n",
        "w2 = wordnet.synset('boat.n.01') # n denotes noun\r\n",
        "print(\"The similarity score betwee ship and boat is =\",w1.wup_similarity(w2))\r\n",
        "\r\n",
        "\r\n",
        "print('Set of hyponyms:\\n', syns[0].hyponyms(), '\\n' )\r\n",
        "print('Set of hypernyms:\\n', syns[0].hypernyms(), '\\n' )\r\n",
        "print('Set of part-meronyms:\\n', wordnet.synset('table.n.2').part_meronyms(), '\\n' )\r\n",
        "print('Set of member-holonyms:\\n', wordnet.synset('kitchen.n.01').part_holonyms() , '\\n' )\r\n",
        "print('Set of part-holonyms:\\n', wordnet.synset('course.n.7').part_holonyms(), '\\n' )\r\n",
        "print('Entailment of word Breathe:\\n', wordnet.synset('snore.v.01').entailments(), '\\n' )"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "kansas.n.01\n",
            "\n",
            "\n",
            "Kansas\n",
            "\n",
            "\n",
            "a state in midwestern United States\n",
            "\n",
            "\n",
            "[]\n",
            "\n",
            "\n",
            "The synonyms of kansas are: \n",
            "{'Kaw_River', 'Kansas', 'Kansas_River', 'Kansa', 'KS', 'Sunflower_State'}\n",
            "\n",
            "\n",
            "The antonyms of kansas are: \n",
            "set()\n",
            "\n",
            "\n",
            "The similarity score betwee ship and boat is = 0.9090909090909091\n",
            "Set of hyponyms:\n",
            " [] \n",
            "\n",
            "Set of hypernyms:\n",
            " [] \n",
            "\n",
            "Set of part-meronyms:\n",
            " [Synset('leg.n.03'), Synset('tabletop.n.01'), Synset('tableware.n.01')] \n",
            "\n",
            "Set of member-holonyms:\n",
            " [Synset('dwelling.n.01')] \n",
            "\n",
            "Set of part-holonyms:\n",
            " [Synset('meal.n.01')] \n",
            "\n",
            "Entailment of word Breathe:\n",
            " [Synset('sleep.v.01')] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRKK_TdCVDWv",
        "outputId": "db13a4d0-c845-4127-d01b-873a648321b6"
      },
      "source": [
        "{\r\n",
        " \"cells\": [\r\n",
        "  {\r\n",
        "   \"cell_type\": \"Run\",\r\n",
        "   \"execution_count\": 1,\r\n",
        "   \"metadata\": {},\r\n",
        "   \"outputs\": [],\r\n",
        "   \"source\": [\r\n",
        "    \"import re\\n\",\r\n",
        "    \"import pandas as pd\\n\",\r\n",
        "    \"import bs4\\n\",\r\n",
        "    \"import requests\\n\",\r\n",
        "    \"import spacy\\n\",\r\n",
        "    \"from spacy import displacy\\n\",\r\n",
        "    \"nlp = spacy.load('en_Run_web_sm')\\n\",\r\n",
        "    \"\\n\",\r\n",
        "    \"from spacy.matcher import Matcher \\n\",\r\n",
        "    \"from spacy.tokens import Span \\n\",\r\n",
        "    \"\\n\",\r\n",
        "    \"import networkx as nx\\n\",\r\n",
        "    \"\\n\",\r\n",
        "    \"import matplotlib.pyplot as plt\\n\",\r\n",
        "    \"from tqdm import tqdm\\n\",\r\n",
        "    \"\\n\",\r\n",
        "    \"pd.set_option('display.max_colwidth', 200)\\n\",\r\n",
        "    \"%matplotlib inline\"\r\n",
        "   ]\r\n",
        "  },\r\n",
        "  {\r\n",
        "   \"cell_type\": \"Run\",\r\n",
        "   \"execution_count\": 2,\r\n",
        "   \"metadata\": {},\r\n",
        "   \"outputs\": [],\r\n",
        "   \"source\": [\r\n",
        "    \"# sample sentences\\n\",\r\n",
        "    \"candidate_sentences = \\\"the drawdown process is governed by astm standard d823\\\"\\n\",\r\n",
        "    \"doc = nlp(candidate_sentences)\"\r\n",
        "   ]\r\n",
        "  },\r\n",
        "  {\r\n",
        "   \"cell_type\": \"Run\",\r\n",
        "   \"execution_count\": 4,\r\n",
        "   \"metadata\": {},\r\n",
        "   \"outputs\": [\r\n",
        "    {\r\n",
        "     \"name\": \"stdout\",\r\n",
        "     \"output_type\": \"stream\",\r\n",
        "     \"text\": [\r\n",
        "      \"the ... det\\n\",\r\n",
        "      \"drawdown ... amod\\n\",\r\n",
        "      \"process ... nsubjpass\\n\",\r\n",
        "      \"is ... auxpass\\n\",\r\n",
        "      \"governed ... ROOT\\n\",\r\n",
        "      \"by ... agent\\n\",\r\n",
        "      \"astm ... compound\\n\",\r\n",
        "      \"standard ... amod\\n\",\r\n",
        "      \"d823 ... pobj\\n\"\r\n",
        "     ]\r\n",
        "    }\r\n",
        "   ],\r\n",
        "   \"source\": [\r\n",
        "    \"for tok in doc:\\n\",\r\n",
        "    \"    print(tok.text, \\\"...\\\", tok.dep_)\"\r\n",
        "   ]\r\n",
        "  },\r\n",
        "  {\r\n",
        "   \"cell_type\": \"markdown\",\r\n",
        "   \"metadata\": {},\r\n",
        "   \"source\": [\r\n",
        "    \"### Entity Pairs Extraction\"\r\n",
        "   ]\r\n",
        "  },\r\n",
        "  {\r\n",
        "   \"cell_type\": \"Run\",\r\n",
        "   \"execution_count\": 5,\r\n",
        "   \"metadata\": {},\r\n",
        "   \"outputs\": [],\r\n",
        "   \"source\": [\r\n",
        "    \"def get_entities(sent):\\n\",\r\n",
        "    \"  ## chunk 1\\n\",\r\n",
        "    \"  ent1 = \\\"\\\"\\n\",\r\n",
        "    \"  ent2 = \\\"\\\"\\n\",\r\n",
        "    \"\\n\",\r\n",
        "    \"  prv_tok_dep = \\\"\\\"    # dependency tag of previous token in the sentence\\n\",\r\n",
        "    \"  prv_tok_text = \\\"\\\"   # previous token in the sentence\\n\",\r\n",
        "    \"\\n\",\r\n",
        "    \"  prefix = \\\"\\\"\\n\",\r\n",
        "    \"  modifier = \\\"\\\"\\n\",\r\n",
        "    \"\\n\",\r\n",
        "    \"  #############################################################\\n\",\r\n",
        "    \"  \\n\",\r\n",
        "    \"  for tok in nlp(sent):\\n\",\r\n",
        "    \"    ## chunk 2\\n\",\r\n",
        "    \"    # if token is a punctuation mark then move on to the next token\\n\",\r\n",
        "    \"    if tok.dep_ != \\\"punct\\\":\\n\",\r\n",
        "    \"      # check: token is a compound word or not\\n\",\r\n",
        "    \"      if tok.dep_ == \\\"compound\\\":\\n\",\r\n",
        "    \"        prefix = tok.text\\n\",\r\n",
        "    \"        # if the previous word was also a 'compound' then add the current word to it\\n\",\r\n",
        "    \"        if prv_tok_dep == \\\"compound\\\":\\n\",\r\n",
        "    \"          prefix = prv_tok_text + \\\" \\\"+ tok.text\\n\",\r\n",
        "    \"      \\n\",\r\n",
        "    \"      # check: token is a modifier or not\\n\",\r\n",
        "    \"      if tok.dep_.endswith(\\\"mod\\\") == True:\\n\",\r\n",
        "    \"        modifier = tok.text\\n\",\r\n",
        "    \"        # if the previous word was also a 'compound' then add the current word to it\\n\",\r\n",
        "    \"        if prv_tok_dep == \\\"compound\\\":\\n\",\r\n",
        "    \"          modifier = prv_tok_text + \\\" \\\"+ tok.text\\n\",\r\n",
        "    \"      \\n\",\r\n",
        "    \"      ## chunk 3\\n\",\r\n",
        "    \"      if tok.dep_.find(\\\"subj\\\") == True:\\n\",\r\n",
        "    \"        ent1 = modifier +\\\" \\\"+ prefix + \\\" \\\"+ tok.text\\n\",\r\n",
        "    \"        prefix = \\\"\\\"\\n\",\r\n",
        "    \"        modifier = \\\"\\\"\\n\",\r\n",
        "    \"        prv_tok_dep = \\\"\\\"\\n\",\r\n",
        "    \"        prv_tok_text = \\\"\\\"      \\n\",\r\n",
        "    \"\\n\",\r\n",
        "    \"      ## chunk 4\\n\",\r\n",
        "    \"      if tok.dep_.find(\\\"obj\\\") == True:\\n\",\r\n",
        "    \"        ent2 = modifier +\\\" \\\"+ prefix +\\\" \\\"+ tok.text\\n\",\r\n",
        "    \"        \\n\",\r\n",
        "    \"      ## chunk 5  \\n\",\r\n",
        "    \"      # update variables\\n\",\r\n",
        "    \"      prv_tok_dep = tok.dep_\\n\",\r\n",
        "    \"      prv_tok_text = tok.text\\n\",\r\n",
        "    \"  #############################################################\\n\",\r\n",
        "    \"\\n\",\r\n",
        "    \"  return [ent1.strip(), ent2.strip()]\"\r\n",
        "   ]\r\n",
        "  },\r\n",
        "  {\r\n",
        "   \"cell_type\": \"Run\",\r\n",
        "   \"execution_count\": 6,\r\n",
        "   \"metadata\": {},\r\n",
        "   \"outputs\": [\r\n",
        "    {\r\n",
        "     \"data\": {\r\n",
        "      \"text/plain\": [\r\n",
        "       \"['film', '200  patents']\"\r\n",
        "      ]\r\n",
        "     },\r\n",
        "     \"execution_count\": 6,\r\n",
        "     \"metadata\": {},\r\n",
        "     \"output_type\": \"execute_result\"\r\n",
        "    }\r\n",
        "   ],\r\n",
        "   \"source\": [\r\n",
        "    \"get_entities(\\\"the film had 200 patents\\\")\"\r\n",
        "   ]\r\n",
        "  },\r\n",
        "  {\r\n",
        "   \"cell_type\": \"markdown\",\r\n",
        "   \"metadata\": {},\r\n",
        "   \"source\": [\r\n",
        "    \"### Entity Relation Extraction\"\r\n",
        "   ]\r\n",
        "  },\r\n",
        "  {\r\n",
        "   \"cell_type\": \"Run\",\r\n",
        "   \"execution_count\": 7,\r\n",
        "   \"metadata\": {},\r\n",
        "   \"outputs\": [],\r\n",
        "   \"source\": [\r\n",
        "    \"def get_relation(sent):\\n\",\r\n",
        "    \"\\n\",\r\n",
        "    \"  doc = nlp(sent)\\n\",\r\n",
        "    \"\\n\",\r\n",
        "    \"  # Matcher class object \\n\",\r\n",
        "    \"  matcher = Matcher(nlp.vocab)\\n\",\r\n",
        "    \"\\n\",\r\n",
        "    \"  #define the pattern \\n\",\r\n",
        "    \"  pattern = [{'DEP':'ROOT'}, \\n\",\r\n",
        "    \"            {'DEP':'prep','OP':\\\"?\\\"},\\n\",\r\n",
        "    \"            {'DEP':'agent','OP':\\\"?\\\"},  \\n\",\r\n",
        "    \"            {'POS':'ADJ','OP':\\\"?\\\"}] \\n\",\r\n",
        "    \"\\n\",\r\n",
        "    \"  matcher.add(\\\"matching_1\\\", None, pattern) \\n\",\r\n",
        "    \"\\n\",\r\n",
        "    \"  matches = matcher(doc)\\n\",\r\n",
        "    \"  k = len(matches) - 1\\n\",\r\n",
        "    \"\\n\",\r\n",
        "    \"  span = doc[matches[k][1]:matches[k][2]] \\n\",\r\n",
        "    \"\\n\",\r\n",
        "    \"  return(span.text)\"\r\n",
        "   ]\r\n",
        "  },\r\n",
        "  {\r\n",
        "   \"cell_type\": \"Run\",\r\n",
        "   \"execution_count\": 8,\r\n",
        "   \"metadata\": {},\r\n",
        "   \"outputs\": [\r\n",
        "    {\r\n",
        "     \"data\": {\r\n",
        "      \"text/plain\": [\r\n",
        "       \"'completed'\"\r\n",
        "      ]\r\n",
        "     },\r\n",
        "     \"execution_count\": 8,\r\n",
        "     \"metadata\": {},\r\n",
        "     \"output_type\": \"execute_result\"\r\n",
        "    }\r\n",
        "   ],\r\n",
        "   \"source\": [\r\n",
        "    \"get_relation(\\\"John completed the task\\\")\"\r\n",
        "   ]\r\n",
        "  },\r\n",
        "  {\r\n",
        "   \"cell_type\": \"markdown\",\r\n",
        "   \"metadata\": {},\r\n",
        "   \"source\": [\r\n",
        "    \"### Combining above both for triplets\"\r\n",
        "   ]\r\n",
        "  },\r\n",
        "  {\r\n",
        "   \"cell_type\": \"markdown\",\r\n",
        "   \"metadata\": {},\r\n",
        "   \"source\": [\r\n",
        "    \"#### Example:1\"\r\n",
        "   ]\r\n",
        "  },\r\n",
        "  {\r\n",
        "   \"cell_type\": \"code\",\r\n",
        "   \"execution_count\": 29,\r\n",
        "   \"metadata\": {},\r\n",
        "   \"outputs\": [],\r\n",
        "   \"source\": [\r\n",
        "    \"text=\\\"tony completed the task\\\"\"\r\n",
        "   ]\r\n",
        "  },\r\n",
        "  {\r\n",
        "   \"cell_type\": \"Run\",\r\n",
        "   \"execution_count\": 30,\r\n",
        "   \"metadata\": {},\r\n",
        "   \"outputs\": [],\r\n",
        "   \"source\": [\r\n",
        "    \"ent=get_entities(text)\\n\",\r\n",
        "    \"rel=get_relation(text)\"\r\n",
        "   ]\r\n",
        "  },\r\n",
        "  {\r\n",
        "   \"cell_type\": \"Run\",\r\n",
        "   \"execution_count\": 31,\r\n",
        "   \"metadata\": {},\r\n",
        "   \"outputs\": [\r\n",
        "    {\r\n",
        "     \"data\": {\r\n",
        "      \"text/plain\": [\r\n",
        "       \"['tony', 'task']\"\r\n",
        "      ]\r\n",
        "     },\r\n",
        "     \"execution_count\": 31,\r\n",
        "     \"metadata\": {},\r\n",
        "     \"output_type\": \"execute_result\"\r\n",
        "    }\r\n",
        "   ],\r\n",
        "   \"source\": [\r\n",
        "    \"ent\"\r\n",
        "   ]\r\n",
        "  },\r\n",
        "  {\r\n",
        "   \"cell_type\": \"Run\",\r\n",
        "   \"execution_count\": 32,\r\n",
        "   \"metadata\": {},\r\n",
        "   \"outputs\": [\r\n",
        "    {\r\n",
        "     \"name\": \"stdout\",\r\n",
        "     \"output_type\": \"stream\",\r\n",
        "     \"text\": [\r\n",
        "      \"['tony', 'completed', 'task']\\n\"\r\n",
        "     ]\r\n",
        "    }\r\n",
        "   ],\r\n",
        "   \"source\": [\r\n",
        "    \"new_list=[]\\n\",\r\n",
        "    \"if len(ent)==2:\\n\",\r\n",
        "    \"    for i,n in enumerate(ent):\\n\",\r\n",
        "    \"        #print(i,n)\\n\",\r\n",
        "    \"        if i==1:\\n\",\r\n",
        "    \"            new_list.append(rel) \\n\",\r\n",
        "    \"        else:\\n\",\r\n",
        "    \"            new_list.append(n)\\n\",\r\n",
        "    \"    new_list.append(ent[1])\\n\",\r\n",
        "    \"print(new_list)\\n\",\r\n",
        "    \"    \"\r\n",
        "   ]\r\n",
        "  },\r\n",
        "  {\r\n",
        "   \"cell_type\": \"markdown\",\r\n",
        "   \"metadata\": {},\r\n",
        "   \"source\": [\r\n",
        "    \"#### Example:2\"\r\n",
        "   ]\r\n",
        "  },\r\n",
        "  {\r\n",
        "   \"cell_type\": \"Run\",\r\n",
        "   \"execution_count\": 33,\r\n",
        "   \"metadata\": {},\r\n",
        "   \"outputs\": [\r\n",
        "    {\r\n",
        "     \"name\": \"stdout\",\r\n",
        "     \"output_type\": \"stream\",\r\n",
        "     \"text\": [\r\n",
        "      \"['drawdown  process', 'governed by', 'astm standard astm d823']\\n\"\r\n",
        "     ]\r\n",
        "    }\r\n",
        "   ],\r\n",
        "   \"source\": [\r\n",
        "    \"text=\\\"the drawdown process is governed by astm standard d823\\\"\\n\",\r\n",
        "    \"ent=get_entities(text)\\n\",\r\n",
        "    \"rel=get_relation(text)\\n\",\r\n",
        "    \"new_list=[]\\n\",\r\n",
        "    \"if len(ent)==2:\\n\",\r\n",
        "    \"    for i,n in enumerate(ent):\\n\",\r\n",
        "    \"        #print(i,n)\\n\",\r\n",
        "    \"        if i==1:\\n\",\r\n",
        "    \"            new_list.append(rel) \\n\",\r\n",
        "    \"        else:\\n\",\r\n",
        "    \"            new_list.append(n)\\n\",\r\n",
        "    \"    new_list.append(ent[1])\\n\",\r\n",
        "    \"print(new_list)\"\r\n",
        "   ]\r\n",
        "  }\r\n",
        " ],\r\n",
        " \"metadata\": {\r\n",
        "  \"kernelspec\": {\r\n",
        "   \"display_name\": \"Python 3\",\r\n",
        "   \"language\": \"python\",\r\n",
        "   \"name\": \"python3\"\r\n",
        "  },\r\n",
        "  \"language_info\": {\r\n",
        "   \"codemirror_mode\": {\r\n",
        "    \"name\": \"ipython\",\r\n",
        "    \"version\": 3\r\n",
        "   },\r\n",
        "   \"file_extension\": \".py\",\r\n",
        "   \"mimetype\": \"text/x-python\",\r\n",
        "   \"name\": \"python\",\r\n",
        "   \"nbconvert_exporter\": \"python\",\r\n",
        "   \"pygments_lexer\": \"ipython3\",\r\n",
        "   \"version\": \"3.8.3\"\r\n",
        "  }\r\n",
        " },\r\n",
        " \"nbformat\": 4,\r\n",
        " \"nbformat_minor\": 4\r\n",
        "}\r\n",
        "\r\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cells': [{'cell_type': 'Run',\n",
              "   'execution_count': 1,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['import re\\n',\n",
              "    'import pandas as pd\\n',\n",
              "    'import bs4\\n',\n",
              "    'import requests\\n',\n",
              "    'import spacy\\n',\n",
              "    'from spacy import displacy\\n',\n",
              "    \"nlp = spacy.load('en_Run_web_sm')\\n\",\n",
              "    '\\n',\n",
              "    'from spacy.matcher import Matcher \\n',\n",
              "    'from spacy.tokens import Span \\n',\n",
              "    '\\n',\n",
              "    'import networkx as nx\\n',\n",
              "    '\\n',\n",
              "    'import matplotlib.pyplot as plt\\n',\n",
              "    'from tqdm import tqdm\\n',\n",
              "    '\\n',\n",
              "    \"pd.set_option('display.max_colwidth', 200)\\n\",\n",
              "    '%matplotlib inline']},\n",
              "  {'cell_type': 'Run',\n",
              "   'execution_count': 2,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['# sample sentences\\n',\n",
              "    'candidate_sentences = \"the drawdown process is governed by astm standard d823\"\\n',\n",
              "    'doc = nlp(candidate_sentences)']},\n",
              "  {'cell_type': 'Run',\n",
              "   'execution_count': 4,\n",
              "   'metadata': {},\n",
              "   'outputs': [{'name': 'stdout',\n",
              "     'output_type': 'stream',\n",
              "     'text': ['the ... det\\n',\n",
              "      'drawdown ... amod\\n',\n",
              "      'process ... nsubjpass\\n',\n",
              "      'is ... auxpass\\n',\n",
              "      'governed ... ROOT\\n',\n",
              "      'by ... agent\\n',\n",
              "      'astm ... compound\\n',\n",
              "      'standard ... amod\\n',\n",
              "      'd823 ... pobj\\n']}],\n",
              "   'source': ['for tok in doc:\\n', '    print(tok.text, \"...\", tok.dep_)']},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['### Entity Pairs Extraction']},\n",
              "  {'cell_type': 'Run',\n",
              "   'execution_count': 5,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['def get_entities(sent):\\n',\n",
              "    '  ## chunk 1\\n',\n",
              "    '  ent1 = \"\"\\n',\n",
              "    '  ent2 = \"\"\\n',\n",
              "    '\\n',\n",
              "    '  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\\n',\n",
              "    '  prv_tok_text = \"\"   # previous token in the sentence\\n',\n",
              "    '\\n',\n",
              "    '  prefix = \"\"\\n',\n",
              "    '  modifier = \"\"\\n',\n",
              "    '\\n',\n",
              "    '  #############################################################\\n',\n",
              "    '  \\n',\n",
              "    '  for tok in nlp(sent):\\n',\n",
              "    '    ## chunk 2\\n',\n",
              "    '    # if token is a punctuation mark then move on to the next token\\n',\n",
              "    '    if tok.dep_ != \"punct\":\\n',\n",
              "    '      # check: token is a compound word or not\\n',\n",
              "    '      if tok.dep_ == \"compound\":\\n',\n",
              "    '        prefix = tok.text\\n',\n",
              "    \"        # if the previous word was also a 'compound' then add the current word to it\\n\",\n",
              "    '        if prv_tok_dep == \"compound\":\\n',\n",
              "    '          prefix = prv_tok_text + \" \"+ tok.text\\n',\n",
              "    '      \\n',\n",
              "    '      # check: token is a modifier or not\\n',\n",
              "    '      if tok.dep_.endswith(\"mod\") == True:\\n',\n",
              "    '        modifier = tok.text\\n',\n",
              "    \"        # if the previous word was also a 'compound' then add the current word to it\\n\",\n",
              "    '        if prv_tok_dep == \"compound\":\\n',\n",
              "    '          modifier = prv_tok_text + \" \"+ tok.text\\n',\n",
              "    '      \\n',\n",
              "    '      ## chunk 3\\n',\n",
              "    '      if tok.dep_.find(\"subj\") == True:\\n',\n",
              "    '        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\\n',\n",
              "    '        prefix = \"\"\\n',\n",
              "    '        modifier = \"\"\\n',\n",
              "    '        prv_tok_dep = \"\"\\n',\n",
              "    '        prv_tok_text = \"\"      \\n',\n",
              "    '\\n',\n",
              "    '      ## chunk 4\\n',\n",
              "    '      if tok.dep_.find(\"obj\") == True:\\n',\n",
              "    '        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\\n',\n",
              "    '        \\n',\n",
              "    '      ## chunk 5  \\n',\n",
              "    '      # update variables\\n',\n",
              "    '      prv_tok_dep = tok.dep_\\n',\n",
              "    '      prv_tok_text = tok.text\\n',\n",
              "    '  #############################################################\\n',\n",
              "    '\\n',\n",
              "    '  return [ent1.strip(), ent2.strip()]']},\n",
              "  {'cell_type': 'Run',\n",
              "   'execution_count': 6,\n",
              "   'metadata': {},\n",
              "   'outputs': [{'data': {'text/plain': [\"['film', '200  patents']\"]},\n",
              "     'execution_count': 6,\n",
              "     'metadata': {},\n",
              "     'output_type': 'execute_result'}],\n",
              "   'source': ['get_entities(\"the film had 200 patents\")']},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['### Entity Relation Extraction']},\n",
              "  {'cell_type': 'Run',\n",
              "   'execution_count': 7,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['def get_relation(sent):\\n',\n",
              "    '\\n',\n",
              "    '  doc = nlp(sent)\\n',\n",
              "    '\\n',\n",
              "    '  # Matcher class object \\n',\n",
              "    '  matcher = Matcher(nlp.vocab)\\n',\n",
              "    '\\n',\n",
              "    '  #define the pattern \\n',\n",
              "    \"  pattern = [{'DEP':'ROOT'}, \\n\",\n",
              "    '            {\\'DEP\\':\\'prep\\',\\'OP\\':\"?\"},\\n',\n",
              "    '            {\\'DEP\\':\\'agent\\',\\'OP\\':\"?\"},  \\n',\n",
              "    '            {\\'POS\\':\\'ADJ\\',\\'OP\\':\"?\"}] \\n',\n",
              "    '\\n',\n",
              "    '  matcher.add(\"matching_1\", None, pattern) \\n',\n",
              "    '\\n',\n",
              "    '  matches = matcher(doc)\\n',\n",
              "    '  k = len(matches) - 1\\n',\n",
              "    '\\n',\n",
              "    '  span = doc[matches[k][1]:matches[k][2]] \\n',\n",
              "    '\\n',\n",
              "    '  return(span.text)']},\n",
              "  {'cell_type': 'Run',\n",
              "   'execution_count': 8,\n",
              "   'metadata': {},\n",
              "   'outputs': [{'data': {'text/plain': [\"'completed'\"]},\n",
              "     'execution_count': 8,\n",
              "     'metadata': {},\n",
              "     'output_type': 'execute_result'}],\n",
              "   'source': ['get_relation(\"John completed the task\")']},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['### Combining above both for triplets']},\n",
              "  {'cell_type': 'markdown', 'metadata': {}, 'source': ['#### Example:1']},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': 29,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['text=\"tony completed the task\"']},\n",
              "  {'cell_type': 'Run',\n",
              "   'execution_count': 30,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['ent=get_entities(text)\\n', 'rel=get_relation(text)']},\n",
              "  {'cell_type': 'Run',\n",
              "   'execution_count': 31,\n",
              "   'metadata': {},\n",
              "   'outputs': [{'data': {'text/plain': [\"['tony', 'task']\"]},\n",
              "     'execution_count': 31,\n",
              "     'metadata': {},\n",
              "     'output_type': 'execute_result'}],\n",
              "   'source': ['ent']},\n",
              "  {'cell_type': 'Run',\n",
              "   'execution_count': 32,\n",
              "   'metadata': {},\n",
              "   'outputs': [{'name': 'stdout',\n",
              "     'output_type': 'stream',\n",
              "     'text': [\"['tony', 'completed', 'task']\\n\"]}],\n",
              "   'source': ['new_list=[]\\n',\n",
              "    'if len(ent)==2:\\n',\n",
              "    '    for i,n in enumerate(ent):\\n',\n",
              "    '        #print(i,n)\\n',\n",
              "    '        if i==1:\\n',\n",
              "    '            new_list.append(rel) \\n',\n",
              "    '        else:\\n',\n",
              "    '            new_list.append(n)\\n',\n",
              "    '    new_list.append(ent[1])\\n',\n",
              "    'print(new_list)\\n',\n",
              "    '    ']},\n",
              "  {'cell_type': 'markdown', 'metadata': {}, 'source': ['#### Example:2']},\n",
              "  {'cell_type': 'Run',\n",
              "   'execution_count': 33,\n",
              "   'metadata': {},\n",
              "   'outputs': [{'name': 'stdout',\n",
              "     'output_type': 'stream',\n",
              "     'text': [\"['drawdown  process', 'governed by', 'astm standard astm d823']\\n\"]}],\n",
              "   'source': ['text=\"the drawdown process is governed by astm standard d823\"\\n',\n",
              "    'ent=get_entities(text)\\n',\n",
              "    'rel=get_relation(text)\\n',\n",
              "    'new_list=[]\\n',\n",
              "    'if len(ent)==2:\\n',\n",
              "    '    for i,n in enumerate(ent):\\n',\n",
              "    '        #print(i,n)\\n',\n",
              "    '        if i==1:\\n',\n",
              "    '            new_list.append(rel) \\n',\n",
              "    '        else:\\n',\n",
              "    '            new_list.append(n)\\n',\n",
              "    '    new_list.append(ent[1])\\n',\n",
              "    'print(new_list)']}],\n",
              " 'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
              "   'language': 'python',\n",
              "   'name': 'python3'},\n",
              "  'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
              "   'file_extension': '.py',\n",
              "   'mimetype': 'text/x-python',\n",
              "   'name': 'python',\n",
              "   'nbconvert_exporter': 'python',\n",
              "   'pygments_lexer': 'ipython3',\n",
              "   'version': '3.8.3'}},\n",
              " 'nbformat': 4,\n",
              " 'nbformat_minor': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    }
  ]
}